# The Tale of T0

> We trained a language model called T0 that is able to generalize to unseen natural language tasks and produce meaningful outputs for applications like generating cooking recipes or explaining moral decisions.


### Overview
When large language models such as GPT-3 succeeded in completing downstream tasks without finetuning on these tasks directly, the NLP community was excited about the future of zero-shot learning, where we can apply the models on tasks that don’t even have labeled data or that annotations are hard to obtain. 

At BigScience, we created T0 to future explore the phenomenon of zero-shot learning. We show that if we train a language model on a large set of different tasks, the model can generalize to unseen tasks, and the model


### Chef T0
T0 can provide cooking instructions even though it is not trained on cooking recipes data<sup>1</sup>. It can even generate high-level instructions for **complex recipes**.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

What if you have a dish in mind, but you don’t know its **ingredients**? Don’t worry, Chef T0 got your back!

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

And the reverse works just as well.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

What about **recommendations for national, regional and seasonal dishes**? To Chef T0, they are all a piece of cake.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

Chef T0 is also helpful for eggs-ploration of **similar dishes**.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 


### The World According To T0
T0 is also capable of responding to world knowledge such as human aging, religion, machine learning, and ethics. All the prompts in this section are adapted from Hendrycks et al.’s (2021)<sup>4</sup> dataset, which aims to measure knowledge acquired by a language model during pretraining. Note that we are not presenting a robust evaluation of T0 here. We specifically chose questions without technical jargon to make this blog post more accessible to everyone.

Does T0 understand human aging from both **biological and social** perspective? Let’s find out.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

How about some questions about **religion**?

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

How much does T0 know about **machine learning**? 

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

We also wonder how T0 stands up to **moral scenarios**.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/> 

And let’s spice it up with some questions about **business ethics**.

<img src="uploads/images/example_prompt.png" alt="placeholder" width="400"/>

### Public Accessibility
If you would like to try the T0 model out yourself, you can [download it from the HuggingFace model repository](https://huggingface.co/bigscience/T0pp). For people who don’t have the ability to use such a large model, the [3 billion parameter version](https://huggingface.co/bigscience/T0_3B) may be more accessible.

### Conclusion
This blog post shows that T0 can generate meaningful outputs for questions about cooking and world knowledge, even when relevant datasets are absent during the model training. We are excited to see more novel and creative applications of T0 and further research on zero-shot generalization ability. 

### References
[1] Michał Bień, Michał Gilski, Martyna Maciejewska, Wojciech Taisner, Dawid Wisniewski, and Agnieszka Lawrynowicz. “RecipeNLG: A cooking recipes dataset for semi-structured text generation.” Proceedings of the 13th International Conference on Natural Language Generation, 2020.

[2] BIG-bench collaboration. “Beyond the imitation game: Measuring and extrapolating the capabilities of language models.” In preparation, 2021.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, and 26 others. “Language models are few-shot learners.” In Advances in Neural Information Processing Systems, 2020.

[4] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. “Measuring Massive Multitask Language Understanding.” Proceedings of the International Conference on Learning Representations (ICLR). 2021.

[5] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, and 37 others. “Multitask Prompted Training Enables Zero-Shot Task Generalization.” arXiv:2110.08207, 2021.

### Attempt on iFrame
<div class="hf-container">
    <iframe src="https://api-inference.huggingface.co/models/gpt2" height="315" width="560" allowfullscreen="" frameborder="0">
    </iframe>
</div>